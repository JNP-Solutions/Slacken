/*
 *
 *  * This file is part of Slacken. Copyright (c) 2019-2024 Johan Nyström-Persson.
 *  *
 *  * Slacken is free software: you can redistribute it and/or modify
 *  * it under the terms of the GNU General Public License as published by
 *  * the Free Software Foundation, either version 3 of the License, or
 *  * (at your option) any later version.
 *  *
 *  * Slacken is distributed in the hope that it will be useful,
 *  * but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  * GNU General Public License for more details.
 *  *
 *  * You should have received a copy of the GNU General Public License
 *  * along with Slacken.  If not, see <https://www.gnu.org/licenses/>.
 *
 */

package com.jnpersson.fastdoop

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.Path
import org.apache.hadoop.io.Text
import org.apache.hadoop.mapreduce.lib.input.FileSplit
import org.apache.hadoop.mapreduce.{InputSplit, RecordReader, TaskAttemptContext}

import scala.io.Source

/**
 * FAI (fasta index) record.
 *
 * Example entries in a FAI file:
 *
 * ENA|LR865458|LR865458.1 590561804       75      60      61
 * ENA|LR865459|LR865459.1 685720839       600404651       60      61
 * ENA|LR865460|LR865460.1 490910922       1297554246      60      61
 *
 * @param id Sequence ID
 * @param length length in bps
 * @param start start position (byte offset in file)
 * @param bpsPerLine bps per line
 * @param bytesPerLine bytes per line
 */
final case class FAIRecord(id: String, length: Long, start: Long, bpsPerLine: Int, bytesPerLine: Int) {
  /** byte offset of the final character in this sequence */
  val end = (start + (length / bpsPerLine) * bytesPerLine + (length % bpsPerLine)) - 1
}

/**
 * FASTA file reader that uses a faidx (.fai) file to track sequence locations.
 * .fai indexes can be generated by various tools, for example seqkit:
 * https://github.com/shenwei356/seqkit/
 *
 * This reader can read a mix of full and partial sequences. If the sequence is fully contained in this split,
 * it will be read as a single [[PartialSequence]] record. Otherwise, it will be read as multiple records.
 * Partial sequences can be identified and reassembled using their header (corresponding to sequence ID)
 * and seqPosition fields.
 *
 * Partial sequences are read together with (k-1) bps from the next part to ensure that full k-mers can be processed.
 *
 * The reader for every split must stream the FAI file. Thus, it is not recommended to use this reader for e.g. short
 * reads, or when the maximum size of a sequence is relatively small.
 * [[ShortReadsRecordReader]] and [[FASTQReadsRecordReader]] are better suited to such a task.
 * For reading a single long sequence without a FAI index, [[LongReadsRecordReader]] can be used instead.
 *
 * @see [[IndexedFastaFormat]]
 *
 * @author Johan Nyström-Persson
 *
 * @version 1.0
 */
class IndexedFastaReader extends RecordReader[Text, PartialSequence] {
  //First byte of this split
  private var startByte = 0L

  //Last byte of this split
  private var endByte = 0L

  private var currKey: Text = _

  private var currValue: PartialSequence = _

  private var myInputSplitBuffer = Array[Byte]()

  private var k = 0

  /**
   * FAIRecords corresponding to sequences that we have yet to read
   */
  private var faiRecords: Iterator[FAIRecord] = Iterator.empty
  private var faiUtils: FAIUtils = _

  private var sizeBuffer1 = 0
  private var sizeBuffer2 = 0

  private def setPartialSequencePosition(record: FAIRecord): Unit = {
    /*
    A FAI record provides "bytes per line" and "bps per line".
    For example, if the latter is 61 but the former is 60, then 1 character per line of text is used for newlines.
    Here we calculate the bp position in the nucleotide sequence that this split will start at.
     */

    val seqStartPosition = startByte - record.start
    val row = seqStartPosition / record.bytesPerLine
    val posInLine = seqStartPosition % record.bytesPerLine

    val pos = if (posInLine >= record.bpsPerLine) {
      /*
      The position must be in the region where newline characters end up, so we consider
      ourselves to be on the next line. The position will apply to the next valid nucleotide character after trimming
      whitespace.
       */
      (row + 1) * record.bpsPerLine
    } else {
      //The position is within the normal sequence
      row * record.bpsPerLine + posInLine
    }
    //Convert from 0-based to 1-based sequence position
    currValue.setSeqPosition(pos + 1)
  }

  override def initialize(genericSplit: InputSplit, context: TaskAttemptContext): Unit = {
    val job = context.getConfiguration

    //Used to ensure we read full k-mers (we aim to read k-1 nucleotides from the next split when needed for a
    //partial sequence)
    k = context.getConfiguration.getInt("k", 10)

    val split = genericSplit match {
      case fs: FileSplit => fs
      case _ => throw new Exception("Unexpected split type")
    }

    val path = split.getPath
    startByte = split.getStart
    endByte = startByte + split.getLength - 1
    val inputFile = path.getFileSystem(job).open(path)
    val totalSize = path.getFileSystem(job).getFileStatus(path).getLen

    val faiPath = new Path(path.toString + ".fai")
    faiUtils = new FAIUtils(faiPath, job, startByte, totalSize)
    faiRecords = faiUtils.recordsAtOffset

    //Keep only those FAI records that correspond to sequences in this split
    faiRecords = faiRecords.
      dropWhile(_.end < startByte).takeWhile(_.start <= endByte)

    val inputSplitSize = split.getLength.toInt
    val additionalBytes = k + 2

    //The entire split is read immediately
    myInputSplitBuffer = new Array[Byte](inputSplitSize + additionalBytes)

    sizeBuffer1 = inputFile.read(startByte, myInputSplitBuffer, 0, inputSplitSize)

    if (sizeBuffer1 <= 0) {
      return
    }

    //Additional characters from the next split
    sizeBuffer2 = inputFile.read(startByte + sizeBuffer1, myInputSplitBuffer, sizeBuffer1, additionalBytes)
    inputFile.close()
  }

  private def safeSetBytesToProcess(kmers: Int): Unit = {
    if (kmers < 0) {
      currValue.setBytesToProcess(0)
    } else {
      currValue.setBytesToProcess(kmers)
    }
  }

  override def nextKeyValue(): Boolean = {
    if (!faiRecords.hasNext || sizeBuffer1 <= 0) return false

    val record = faiRecords.next()

    currKey = new Text(record.id)
    currValue = new PartialSequence
    currValue.setHeader(record.id)
    currValue.setBuffer(myInputSplitBuffer)
    if (record.start >= startByte && record.end <= endByte) {
      //Read the sequence in full
      currValue.setSeqPosition(1)
      currValue.setComplete(true)
      currValue.setStartValue((record.start - startByte).toInt)
      currValue.setEndValue((record.end - startByte).toInt)
      //Number of k-mers (and newlines) in the value
      safeSetBytesToProcess((record.end - record.start + 1 - (k - 1)).toInt)
    } else {
      //Read partial
      if (record.start >= startByte) {
        //Started in this split
        currValue.setSeqPosition(1)
        //Skips the sequence header
        currValue.setStartValue((record.start - startByte).toInt)
      } else {
        //Started previously
        setPartialSequencePosition(record)
        currValue.setStartValue(0)
      }
      if (record.end <= endByte) {
        //Sequence ends in this split
        //Excess newline characters at the end will be automatically trimmed
        currValue.setEndValue((record.end - startByte).toInt)
        safeSetBytesToProcess(currValue.getEndValue - currValue.getStartValue + 1 - (k - 1))
      } else {
        //Sequence reaches into next split
        currValue.setEndValue(sizeBuffer1 + sizeBuffer2 - 1)
        //Number of k-mers (mixed with newlines) in the value
        safeSetBytesToProcess(sizeBuffer1 - currValue.getStartValue)

        //If the extra part in the next split was shorter than k - 1, we have to reduce accordingly
        if (sizeBuffer2 < (k - 1)) currValue.setBytesToProcess(
          currValue.getBytesToProcess - ((k - 1) - sizeBuffer2))
      }
    }

    true
  }

  override def getCurrentKey: Text =
    currKey

  override def getCurrentValue: PartialSequence =
    currValue

  override def getProgress: Float =
    if (faiRecords.hasNext) 0 else 1

  override def close(): Unit = {
    //inputFile has already been closed
    faiUtils.close()
  }
}

/** Routines for reading a FASTA index (.fai) file efficiently.
 * They can be generated by e.g. 'seqkit faidx'
 *
 * This algorithm tries to seek to the best position in the FAI file based on the current offset in the
 * fasta file, assuming the records are on average evenly spread out. This is to help performance in the case
 * where FAI files are huge, and we would otherwise need to parse the whole thing for each split.
 * Additional records may be included before or after, so the start position of each record should still be checked.
 *
 * @param path Path to the fai file
 * @param job The Hadoop configuration
 * @param startByte start byte in the fasta file
 * @param fullSize total size of the fasta file
 */
class FAIUtils(path: Path, job: Configuration, startByte: Long, fullSize: Long) {

  def fileSize: Long =
    path.getFileSystem(job).getFileStatus(path).getLen

  /** Stream data from the current offset in the fai file */
  private def sourceAtOffset: Source = {
    val is = path.getFileSystem(job).open(path)
    is.seek(startOffset)
    Source.fromInputStream(is)
  }

  /**
   * Stream FAI records from the current offset in the fai file.
   * After use, the close() method must be called to close the underlying source.
   */
  def recordsAtOffset: BufferedIterator[FAIRecord] = {
    val s = sourceAtOffset
    faiSource = Some(s)

    val lines = s.getLines()
    if (startOffset > 0 && lines.hasNext) {
      lines.next() //ensure we start from a complete line as we might have seeked to the middle of one
    }
    lines.map(line => {
      try {
        val spl = line.split("\t")
        FAIRecord(spl(0), spl(1).toLong, spl(2).toLong, spl(3).toInt, spl(4).toInt)
      } catch {
        case e @ (_: NumberFormatException | _: ArrayIndexOutOfBoundsException) =>
          println(s"Exception while parsing FAI file: record $line")
          throw e
      }
    }).buffered
  }

  //initial guess
  private var startOffset = (fileSize * (startByte.toDouble / fullSize)).toLong
  private var faiSource: Option[Source] = None
  seekToStart()

  //seek backwards if we have gone too far
  private def seekToStart(): Unit = {
    val seekAmount = 1024 * 1024 * 10

    var firstRecordStart = recordsAtOffset.headOption.map(_.start).getOrElse(fileSize)
    while (firstRecordStart > startByte && startOffset > 0) {
      //seek back
      startOffset -= seekAmount
      if (startOffset < 0) startOffset = 0
      close()
      firstRecordStart = recordsAtOffset.headOption.map(_.start).getOrElse(fileSize)
    }
    close()
  }

  def close(): Unit =
    for { s <- faiSource } s.close()

}
