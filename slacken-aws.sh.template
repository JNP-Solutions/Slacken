#!/bin/bash
#Script to submit Slacken as an Amazon AWS EMR step. Copy this file to aws-discount.sh and 
#edit variables accordingly.

#For this script to work, it is necessary to install and configure the AWS CLI.

if [ -z "${AWS_EMR_CLUSTER}" ]
then
  echo "Please set AWS_EMR_CLUSTER"
  exit 1
fi

#Bucket to store Slacken jars and data files
BUCKET=s3://path/to/slacken

DISCOUNT_HOME="$(dirname -- "$(readlink -f "${BASH_SOURCE}")")"

#Copy jars and data files the first time only, after which the following lines can safely be commented out
aws s3 cp $DISCOUNT_HOME/target/scala-2.12/Slacken-assembly-0.1.0.jar $BUCKET/
#aws s3 sync $DISCOUNT_HOME/PASHA $BUCKET/PASHA/

#Max size of input splits in bytes. A smaller number reduces memory usage but increases the number of 
#partitions for the first stage. If this variable is unset, Spark's default of 128 MB will be used.
#SPLIT="spark.hadoop.mapreduce.input.fileinputformat.split.maxsize=$((64 * 1024 * 1024))"

#To run slacken 1, change the main class to ...slacken.Slacken

#To set SPLIT, uncomment below.
#Usually, the default values are fine.
COMMAND=( \
#  --conf $SPLIT \
  --conf spark.driver.memory=24g \
  --class com.jnpersson.slacken.Slacken2 $BUCKET/Slacken-assembly-0.1.0.jar $*)


#Turn off paging for output
export AWS_PAGER=""

RUNNER_ARGS="spark-submit"
for PARAM in ${COMMAND[@]}
do
  RUNNER_ARGS="$RUNNER_ARGS,$PARAM"
done

aws emr add-steps --cluster $AWS_EMR_CLUSTER --steps Type=CUSTOM_JAR,Name=Hypercut,ActionOnFailure=CONTINUE,Jar=command-runner.jar,Args=\[$RUNNER_ARGS\]
